---
# Source: crds/apprepository-crd.yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: apprepositories.kubeapps.com
spec:
  group: kubeapps.com
  scope: Namespaced
  names:
    kind: AppRepository
    plural: apprepositories
    shortNames:
      - apprepos
  version: v1alpha1

---
# Source: kubeapps/templates/apprepository-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
---
# Source: kubeapps/templates/apprepository-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-job-postupgrade
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
---
# Source: kubeapps/templates/kubeops-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-kubeops
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
---
# Source: kubeapps/templates/dashboard-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-internal-dashboard-config
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-dashboard-config
data:
  vhost.conf: |-
    server {
      listen 8080;
      server_name _;

      gzip on;
      gzip_static  on;

      location / {
        # Redirects are required to be relative otherwise the internal hostname will be exposed
        absolute_redirect off;

        # Trailing / is required in the path for the React app to be loaded correctly
        # The rewrite rule adds a trailing "/" to any path that does not contain "." neither "/".
        # i.e kubeapps => kubeapps/
        rewrite ^([^.]*[^/])$ $1/ permanent;

        # Support for ingress prefixes maintaining compatibility with the default /
        # 1 - Exactly two fragment URLs for files existing inside of the public/ dir
        # i.e /[prefix]/config.json => /config.json
        rewrite ^/[^/]+/([^/]+)$ /$1 break;

        # 2 - Any static files bundled by webpack referenced by 3 or more URL segments
        # i.e /[prefix]/static/main.js => static/main.js
        rewrite ^/[^/]+/static/(.*) /static/$1 break;

        try_files $uri /index.html;
      }
    }
  config.json: |-
    {
      "kubeappsCluster": "default",
      "kubeappsNamespace": "kubeapps",
      "appVersion": "v1.11.3",
      "authProxyEnabled": false,
      "oauthLoginURI": "/oauth2/start",
      "oauthLogoutURI": "/oauth2/sign_out",
      "featureFlags": {"invalidateCache":true,"operators":false,"ui":"hex"},
      "clusters": []
    }
---
# Source: kubeapps/templates/kubeapps-frontend-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeapps-frontend-config
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-frontend-config
data:
  k8s-api-proxy.conf: |-
    # Disable buffering for log streaming
    proxy_buffering off;
    # Hide Www-Authenticate to prevent it triggering a basic auth prompt in
    # the browser with some clusters
    proxy_hide_header Www-Authenticate;

    # Keep the connection open with the API server even if idle (the default is 60 seconds)
    # Setting it to 1 hour which should be enough for our current use case of deploying/upgrading apps
    # If we enable other use-cases in the future we might need to bump this value
    # More info here https://github.com/kubeapps/kubeapps/issues/766
    proxy_read_timeout 1h;
  vhost.conf: |-
    # Retain the default nginx handling of requests without a "Connection" header
    map $http_upgrade $connection_upgrade {
      default upgrade;
      ''      close;
    }

    # Allow websocket connections
    proxy_set_header Upgrade    $http_upgrade;
    proxy_set_header Connection $connection_upgrade;

    server {
      listen 8080;
      server_name _;

      location /healthz {
        access_log off;
        default_type text/plain;
        return 200 "healthy\n";
      }

      # The default cluster running on the same cluster as Kubeapps.
      location ~* /api/clusters/default {
        rewrite /api/clusters/default/(.*) /$1 break;
        rewrite /api/clusters/default / break;
        proxy_pass https://kubernetes.default;
        include "./server_blocks/k8s-api-proxy.conf";
      }

      # Ensure each additional cluster can be reached (should only be
      # used with an auth-proxy where k8s credentials never leave
      # the cluster). See clusters option.

      # TODO: The following location is left for backwards compat but will no longer
      # be needed once clients are sending the cluster name.
      # Using regexp match instead of prefix one because the application can be
      # deployed under a specific path i.e /kubeapps
      location ~* /api/kube {
        rewrite /api/kube/(.*) /$1 break;
        rewrite /api/kube / break;
        proxy_pass https://kubernetes.default;
        include "./server_blocks/k8s-api-proxy.conf";
      }

      location ~* /api/assetsvc {
        rewrite /api/assetsvc/(.*) /assetsvc/$1 break;
        rewrite /api/assetsvc /assetsvc break;

        proxy_pass http://kubeapps-internal-kubeops:8080;
      }

      location ~* /api/tiller-deploy {
        # Keep the connection open with the API server even if idle (the default is 60 seconds)
        # Setting it to 10 minutes which should be enough for our current use case of deploying/upgrading/deleting apps
        proxy_read_timeout 10m;
        rewrite /api/tiller-deploy/(.*) /$1 break;
        rewrite /api/tiller-deploy / break;
        proxy_pass http://kubeapps-internal-kubeops:8080;
      }

      # The route for the Kubeapps backend API is not prefixed.
      location ~* /api/ {
        rewrite /api/(.*) /backend/$1 break;
        rewrite /api/ /backend break;

        proxy_pass http://kubeapps-internal-kubeops:8080;
      }

      location / {
        # Add the Authorization header if exists
        add_header Authorization $http_authorization;

        proxy_pass http://kubeapps-internal-dashboard:8080;
      }
    }
---
# Source: kubeapps/templates/apprepository-rbac.yaml
# The Kubeapps app repository controller can read and watch its own
# AppRepository resources cluster-wide. The read and write cluster-roles can
# also be bound to users in specific namespaces as required.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: "kubeapps:kubeapps:apprepositories-read"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
      - apprepositories/finalizers
    verbs:
      - get
      - list
      - watch
---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: "kubeapps:kubeapps:apprepositories-write"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - '*'
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
# Source: kubeapps/templates/kubeops-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: "kubeapps:controller:kubeops-ns-discovery-kubeapps"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - list
---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: "kubeapps:controller:kubeapps:apprepositories-read"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: "kubeapps:kubeapps:apprepositories-read"
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-controller
    namespace: kubeapps
---
# Source: kubeapps/templates/kubeops-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: "kubeapps:controller:kubeops-ns-discovery-kubeapps"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: "kubeapps:controller:kubeops-ns-discovery-kubeapps"
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-kubeops
    namespace: kubeapps
---
# Source: kubeapps/templates/apprepository-jobs-postupgrade-rbac.yaml
# Helm 3.1 supports a lookup template tag to create a secret if it does not exist
# but we can't yet restrict to helm 3.1, hence manually doing this with an initContainer.
# in the post upgrade job.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-job-postupgrade
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - batch
    resources:
      - cronjobs
    verbs:
      - create
      - get
      - list
      - update
      - watch
      - delete
  - apiGroups:
      - batch
    resources:
      - jobs
    verbs:
      - create
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
      - apprepositories/finalizers
    verbs:
      - get
      - list
      - update
      - watch
---
# Source: kubeapps/templates/apprepository-rbac.yaml
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-repositories-read
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - list
      - get
---
# Source: kubeapps/templates/apprepository-rbac.yaml
# Define role, but no binding, so users can be bound to this role
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-repositories-write
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - "*"
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - create
---
# Source: kubeapps/templates/kubeops-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-internal-kubeops
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
      - delete
  - apiGroups:
      - "kubeapps.com"
    resources:
      - apprepositories
    verbs:
      - get
---
# Source: kubeapps/templates/apprepository-jobs-postupgrade-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-job-postupgrade
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-job-postupgrade
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-job-postupgrade
    namespace: kubeapps
---
# Source: kubeapps/templates/apprepository-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-controller
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-controller
    namespace: kubeapps
---
# Source: kubeapps/templates/kubeops-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubeapps-internal-kubeops
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-kubeops
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-kubeops
    namespace: kubeapps
---
# Source: kubeapps/charts/postgresql/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-postgresql-headless
  labels:
    app: postgresql
    chart: postgresql-8.10.14
    release: "kubeapps"
    heritage: "Helm"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "kubeapps"
---
# Source: kubeapps/charts/postgresql/templates/svc-read.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-postgresql-read
  labels:
    app: postgresql
    chart: postgresql-8.10.14
    release: "kubeapps"
    heritage: "Helm"
  annotations:
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port:  5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "kubeapps"
    role: slave
---
# Source: kubeapps/charts/postgresql/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-postgresql
  labels:
    app: postgresql
    chart: postgresql-8.10.14
    release: "kubeapps"
    heritage: "Helm"
  annotations:
spec:
  type: ClusterIP
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app: postgresql
    release: "kubeapps"
    role: master
---
# Source: kubeapps/templates/assetsvc-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-assetsvc
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-assetsvc
    release: kubeapps
---
# Source: kubeapps/templates/dashboard-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-dashboard
  labels:
    app: kubeapps
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-dashboard
    release: kubeapps
---
# Source: kubeapps/templates/kubeapps-frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps
  labels:
    app: kubeapps
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps
    release: kubeapps
---
# Source: kubeapps/templates/kubeops-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubeapps-internal-kubeops
  labels:
    app: kubeapps
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: kubeapps-internal-kubeops
    release: kubeapps
---
# Source: kubeapps/templates/apprepository-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-apprepository-controller
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kubeapps-internal-apprepository-controller
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-controller
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
        app.kubernetes.io/name: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-apprepository-controller
      containers:
        - name: controller
          image: docker.io/bitnami/kubeapps-apprepository-controller:1.11.3-scratch-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /apprepository-controller
          args:
            - --user-agent-comment=kubeapps/v1.11.3
            - --repo-sync-image=docker.io/bitnami/kubeapps-asset-syncer:1.11.3-scratch-r0
            - --repo-sync-cmd=/asset-syncer
            - --namespace=kubeapps
            - --database-secret-name=kubeapps-db
            - --database-secret-key=postgresql-password
            - --database-type=postgresql
            - --database-url=kubeapps-postgresql:5432
            - --database-user=postgres
            - --database-name=assets
            - --repos-per-namespace
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
---
# Source: kubeapps/templates/assetsvc-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-assetsvc
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-assetsvc
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-assetsvc
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-assetsvc
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      containers:
        - name: assetsvc
          image: docker.io/bitnami/kubeapps-assetsvc:1.11.3-scratch-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /assetsvc
          args:
            - --database-type=postgresql
            - --database-user=postgres
            - --database-name=assets
            - --database-url=kubeapps-postgresql-headless:5432
          env:
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-db
                  key: postgresql-password
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
---
# Source: kubeapps/templates/dashboard-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-dashboard
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-dashboard
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-dashboard
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: 8b7b7bc5908b5deff15034b9644daa036e1fab6b813b5e7b4830e7c21fa7b901
      labels:
        app: kubeapps-internal-dashboard
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
        chart: kubeapps-3.9.2
        helm.sh/chart: kubeapps-3.9.2
    spec:
      containers:
        - name: dashboard
          image: docker.io/bitnami/kubeapps-dashboard:1.11.3-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
            - name: config
              mountPath: /app/config.json
              subPath: config.json
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
      volumes:
        - name: vhost
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: vhost.conf
                path: vhost.conf
        - name: config
          configMap:
            name: kubeapps-internal-dashboard-config
            items:
              - key: config.json
                path: config.json
---
# Source: kubeapps/templates/kubeapps-frontend-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps
      release: kubeapps
  template:
    metadata:
      annotations:
        checksum/config: 976e166f43c29fd0a1f6f87c5d0f78990e40bbd73a8965e7b966d44aac8d176e
      labels:
        app: kubeapps
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      containers:
        - name: nginx
          image: docker.io/bitnami/nginx:1.19.2-debian-10-r1
          imagePullPolicy: "IfNotPresent"
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
          volumeMounts:
            - name: vhost
              mountPath: /opt/bitnami/nginx/conf/server_blocks
          ports:
            - name: http
              containerPort: 8080
          resources:
            limits:
              cpu: 250m
              memory: 128Mi
            requests:
              cpu: 25m
              memory: 32Mi
      volumes:
        - name: vhost
          configMap:
            name: kubeapps-frontend-config
---
# Source: kubeapps/templates/kubeops-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeapps-internal-kubeops
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-kubeops
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeapps-internal-kubeops
      release: kubeapps
  template:
    metadata:
      labels:
        app: kubeapps-internal-kubeops
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      serviceAccountName: kubeapps-internal-kubeops
      # Increase termination timeout to let remaining operations to finish before killing the pods
      # This is because new releases/upgrades/deletions are synchronous operations
      terminationGracePeriodSeconds: 300
      containers:
        - name: kubeops
          image: docker.io/bitnami/kubeapps-kubeops:1.11.3-scratch-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /kubeops
          args:
            - --user-agent-comment=kubeapps/v1.11.3
            - --assetsvc-url=http://kubeapps-internal-assetsvc:8080
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 8080
          livenessProbe:
            httpGet:
              path: /live
              port: 8080
            initialDelaySeconds: 60
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 0
            timeoutSeconds: 5
          resources:
            limits:
              cpu: 250m
              memory: 256Mi
            requests:
              cpu: 25m
              memory: 32Mi
---
# Source: kubeapps/charts/postgresql/templates/statefulset-slaves.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "kubeapps-postgresql-slave"
  labels:
    app: postgresql
    chart: postgresql-8.10.14
    release: "kubeapps"
    heritage: "Helm"
  annotations:
spec:
  serviceName: kubeapps-postgresql-headless
  replicas: 1
  selector:
    matchLabels:
      app: postgresql
      release: "kubeapps"
      role: slave
  template:
    metadata:
      name: kubeapps-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.10.14
        release: "kubeapps"
        heritage: "Helm"
        role: slave
    spec:
      initContainers:
        # - name: do-something
        #   image: busybox
        #   command: ['do', 'something']

      containers:
        - name: kubeapps-postgresql
          image: docker.io/bitnami/postgresql:11.8.0-debian-10-r57
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_REPLICATION_MODE
              value: "slave"
            - name: POSTGRES_REPLICATION_USER
              value: "repl_user"
            - name: POSTGRES_REPLICATION_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-db
                  key: postgresql-replication-password
            - name: POSTGRES_CLUSTER_APP_NAME
              value: my_application
            - name: POSTGRES_MASTER_HOST
              value: kubeapps-postgresql
            - name: POSTGRES_MASTER_PORT_NUMBER
              value: "5432"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-db
                  key: postgresql-password
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "assets" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "assets" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: data
          emptyDir: {}
  updateStrategy:
    type: RollingUpdate
---
# Source: kubeapps/charts/postgresql/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kubeapps-postgresql-master
  labels:
    app: postgresql
    chart: postgresql-8.10.14
    release: "kubeapps"
    heritage: "Helm"
  annotations:
spec:
  serviceName: kubeapps-postgresql-headless
  replicas: 1
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: postgresql
      release: "kubeapps"
      role: master
  template:
    metadata:
      name: kubeapps-postgresql
      labels:
        app: postgresql
        chart: postgresql-8.10.14
        release: "kubeapps"
        heritage: "Helm"
        role: master
    spec:
      containers:
        - name: kubeapps-postgresql
          image: docker.io/bitnami/postgresql:11.8.0-debian-10-r57
          imagePullPolicy: "IfNotPresent"
          resources:
            requests:
              cpu: 250m
              memory: 256Mi
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            - name: POSTGRES_REPLICATION_MODE
              value: "master"
            - name: POSTGRES_REPLICATION_USER
              value: "repl_user"
            - name: POSTGRES_REPLICATION_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-db
                  key: postgresql-replication-password
            - name: POSTGRES_CLUSTER_APP_NAME
              value: my_application
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: kubeapps-db
                  key: postgresql-password
            - name: POSTGRES_DB
              value: "assets"
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "postgres" -d "assets" -h 127.0.0.1 -p 5432
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                - |
                  exec pg_isready -U "postgres" -d "assets" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
        - name: data
          emptyDir: {}
---
# Source: kubeapps/templates/apprepository-jobs-cleanup-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-jobs-cleanup
---
# Source: kubeapps/templates/db-secret-jobs-cleanup-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubeapps-internal-db-secret-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-db-secret-jobs-cleanup
---
# Source: kubeapps/templates/db-secret-bootstrap.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubeapps-db
  annotations:
    helm.sh/hook: pre-install
  labels:
    chart: kubeapps-3.9.2
    helm.sh/chart: kubeapps-3.9.2
    release: kubeapps
    app.kubernetes.io/instance: kubeapps
    heritage: Helm
    app.kubernetes.io/managed-by: Helm
data:
  postgresql-password: "MFN3Zktvb3EzVQ=="
  postgresql-replication-password: "ZlNqM1MzTjBDWA=="
---
# Source: kubeapps/templates/apprepository-jobs-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-jobs-cleanup
rules:
  - apiGroups:
      - kubeapps.com
    resources:
      - apprepositories
    verbs:
      - list
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - list
      - delete
---
# Source: kubeapps/templates/db-secret-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubeapps-internal-db-secret-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-db-secret-jobs-cleanup
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - delete
---
# Source: kubeapps/templates/apprepository-jobs-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-jobs-cleanup
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-apprepository-jobs-cleanup
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-apprepository-jobs-cleanup
    namespace: kubeapps
---
# Source: kubeapps/templates/db-secret-cleanup-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubeapps-internal-db-secret-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
    helm.sh/hook-weight: "-10"
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-db-secret-jobs-cleanup
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubeapps-internal-db-secret-jobs-cleanup
subjects:
  - kind: ServiceAccount
    name: kubeapps-internal-db-secret-jobs-cleanup
    namespace: kubeapps
---
# Source: kubeapps/templates/tests/test-assetsvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "kubeapps-assetsvc-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: kubeapps-assetsvc-test
      image: docker.io/bitnami/nginx:1.19.2-debian-10-r1
      env:
        - name: ASSETSVC_HOST
          value: kubeapps-internal-assetsvc.kubeapps
        - name: ASSETSVC_PORT
          value: "8080"
      command:
        - bash
        - -c
        # We are requesting the "wordpress" chart to the chartsvc to check that everything works as expected
        # Also, we retry several times in case the repository is being populated at the time of executing this test
        - |
            n=0
            until [ "$n" -ge 5 ]; do
              if curl -o /tmp/output $ASSETSVC_HOST:$ASSETSVC_PORT/v1/ns/kubeapps/charts && cat /tmp/output && cat /tmp/output | grep wordpress; then
                break
              fi
              sleep 10
              ((n+=1))
            done
            if [ "$n" -eq 5 ]; then
              exit 1
            fi
  restartPolicy: Never
---
# Source: kubeapps/templates/tests/test-dashboard.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "kubeapps-dashboard-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
    - name: kubeapps-dashboard-test
      image: docker.io/bitnami/nginx:1.19.2-debian-10-r1
      env:
        - name: DASHBOARD_HOST
          value: kubeapps.kubeapps
      command:
        - sh
        - -c
        - curl -o /tmp/output $DASHBOARD_HOST && cat /tmp/output && cat /tmp/output | grep 'You need to enable JavaScript to run this app'
  restartPolicy: Never
---
# Source: kubeapps/templates/apprepository-jobs-cleanup.yaml
# Clean up the AppRepository resources used by this Kubeapps instance
apiVersion: batch/v1
kind: Job
metadata:
  name: kubeapps-internal-apprepository-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-jobs-cleanup
spec:
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-jobs-cleanup
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      restartPolicy: OnFailure
      serviceAccountName: kubeapps-internal-apprepository-jobs-cleanup
      containers:
        - name: kubectl
          image: docker.io/bitnami/kubectl:1.16.14-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/sh
          args:
            - -ec
            - |
              kubectl delete apprepositories.kubeapps.com -n kubeapps --all
              kubectl delete secrets -n kubeapps -l app=kubeapps,release=kubeapps
---
# Source: kubeapps/templates/apprepository-jobs-postupgrade.yaml
# Ensure db indexes are set and invalidate the chart when upgrading.
apiVersion: batch/v1
kind: Job
metadata:
  name: kubeapps-internal-apprepository-job-postupgrade
  annotations:
    helm.sh/hook: post-upgrade
    helm.sh/hook-weight: "0"
    helm.sh/hook-delete-policy: hook-succeeded
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-job-postupgrade
spec:
  template:
    metadata:
      labels:
        app: kubeapps-internal-apprepository-job-postupgrade
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      restartPolicy: OnFailure
      serviceAccountName: kubeapps-internal-apprepository-job-postupgrade
      containers:
        - name: invalidate-cache
          image: docker.io/bitnami/kubeapps-asset-syncer:1.11.3-scratch-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /asset-syncer
          args:
            - invalidate-cache
            - --database-type=postgresql
            - --database-url=kubeapps-postgresql:5432
            - --database-user=postgres
            - --database-name=assets
          env:
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  key: postgresql-password
                  name: kubeapps-db
      initContainers:
        # Helm 3.1 supports a lookup template tag to create a secret if it does not exist
        # but we can't yet restrict to helm 3.1, hence manually doing this with an initContainer.
        - name: ensure-postgres-password
          image: docker.io/bitnami/kubectl:1.16.14-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/bash
          args:
            - -c
            - 'kubectl -n kubeapps get secret kubeapps-db || kubectl -n kubeapps create secret generic kubeapps-db --from-literal=postgresql-password="R2szGs7NrZ" --from-literal=postgresql-replication-password="aITuzFgY63"'
---
# Source: kubeapps/templates/db-secret-jobs-cleanup.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: kubeapps-internal-db-secret-jobs-cleanup
  annotations:
    helm.sh/hook: post-delete
    helm.sh/hook-delete-policy: hook-succeeded
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-db-secret-jobs-cleanup
spec:
  template:
    metadata:
      labels:
        app: kubeapps-internal-db-secret-jobs-cleanup
        app.kubernetes.io/name: kubeapps
        release: kubeapps
        app.kubernetes.io/instance: kubeapps
    spec:
      restartPolicy: OnFailure
      serviceAccountName: kubeapps-internal-db-secret-jobs-cleanup
      containers:
        - name: kubectl
          image: docker.io/bitnami/kubectl:1.16.14-debian-10-r0
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/sh
          args:
            - -c
            - "kubectl delete secret -n kubeapps kubeapps-mongodb kubeapps-db|| true"
---
# Source: kubeapps/templates/apprepositories.yaml
apiVersion: kubeapps.com/v1alpha1
kind: AppRepository
metadata:
  name: bitnami
  annotations:
    "helm.sh/hook": post-install
  labels:
    chart: kubeapps-3.9.2
    release: kubeapps
    heritage: Helm
    helm.sh/chart: kubeapps-3.9.2
    app.kubernetes.io/instance: kubeapps
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubeapps
    app: kubeapps-internal-apprepository-controller
spec:
  type: helm
  url: https://charts.bitnami.com/bitnami